---
title: "R4DSNotes"
author: "Yang Ge"
date: "7/25/2017"
output:
  html_document:
    toc: true
---

```{r}
library(tidyverse)
library(nycflights13)
```
# 5 Data transformation

## 5.1 Introduction

* filter, pick observations
* arrange, reorder the row
* select, pick the variables by name
* mutate, create variable with functions of existing variables
* summarise, Collapse many values down to a single summary
* group_by, used in conjunction with the above, apply it on different groups

Use them similarly:

1. The first arg is a dataframe
2. The subsequent arguments describe what to do with the DF, variable names without quote
3. result is a new DF.

## 5.2 filter() notes
* If we want to both assign to a variable and print out, use ()
* Comparison operators: `>, >=, <, <=, !=, ==`
    * when compare numerical, use `near(sqrt(2)^2, 2)`
* Logic operators: `&, |, !`
    * handy shorthand: `x %in% y`
* Almost all operations with `NA` is `NA`.
* Filter only includes rows where the condition is `TRUE`, excludes both `NA` and `FALSE`

## 5.3 arrange() notes
* arrange() change the order of the rows.
* use desc() to reverse the order.
* NA is always at the end, no matter ascending or desending

## 5.4 select() notes

* Select couple of variables
* Have a couple of helper functions
    * Like `starts_with` and `matches`, see ?select_helpers
* rename function to change variable name
* everthing helper function to bring a variable to the front

## 5.5 mutate() notes

* `mutate()` can use the variable just created
* `transmute()` only keeps the new variables
* Useful creation functions, take in vectors and produce vectors
    * Arithmetic operators: +/-/*/ / /^
    * Arithmetic operators with aggregate functions: x/sum(x)
    * module %/%, %%
    * logs
    * offset: lead/lag, useful in computing running difference or find when value changes.
    * cumulative function
    * ranking functions: min_rank() and etc. see ?min_rank

## 5.6 summarise() notes

* collapse a data frame to a single row
* More useful with group_by
* Use pipe %>% is much easier to coding and reading
    * x %>% f(y) %>% g(z) <=> g(f(x, y), z)
    * example here:

```
delays <- flights %>% 
  group_by(dest) %>% 
  summarise(
    count = n(),
    dist = mean(distance, na.rm = TRUE),
    delay = mean(arr_delay, na.rm = TRUE)
  ) %>% 
  filter(count > 20, dest != "HNL")
```

* missing value, mean function has `na.rm`.
    * But you can also do filter like

```{r}
not_cancelled <- flights %>%
  filter(!is.na(dep_delay), !is.na(arr_delay))
```

* When looking at this sort of plot, itâ€™s often useful to filter out the groups with the smallest numbers of observations, so you can see more of the pattern and less of the extreme variation in the smallest groups.

```{r, echo=FALSE}
delays <- not_cancelled %>%
  group_by(tailnum) %>%
  summarise(
    delay = mean(arr_delay, na.rm = TRUE),
    n = n()
  )
delays %>%
  filter(n > 100) %>%
  ggplot(delays, mapping = aes(x = n, y = delay)) +
  geom_point(alpha = 1/10)
```

### 5.6.4 Useful summary functions

* locations: `mean()` and `median()`, and can be combined with logical subsetting.
* spread: `sd(), IQR(), mad(x)`
* rank: `min(x), max(x), quantile(x, 0.25)`
* position: `first(), nth(x, 2), last()`
    * These can be complementary to filtering on ranks.
* count: `n(), n_distinct(), count()`
    * count can add a weight `count(tailnum, wt = distance)`.
* counts and proportions of logical values: `sum(x) > 10`, `mean(y == 0)`.

### 5.6.5 Grouping by multiple variables

* When you group by multiple variables, each summary peels off one level of the grouping.
* remove group by 'ungroup'

## 5.7 Grouped mutates (and filters)

* grouping can be also used with `mutate()` and `filter()`.
* window functions work most naturelly in grouped mutates and filters. Try `vignette("window-functions")`.

# 6 Workflow: scripts

## 6.1 Running code

* `Cmd+Enter` to run the current line
* `Cmd+Shift+S` to run the current script
* never put `install.package()` or `setwd()` in the script to share with others.

# 7 Exploratory Data Analysis

## 7.1 Introduction

EDA means to use visualization and transformation to explore your data in a systematic way.
EDA iterations have the following 3 steps:

* Generate questions about your data
* Search for answers by visualizing, transforming, and modeling your data
* Use what you learn to refine your questions and/or generate new questions

## 7.2 Questions

* The key to ask quality question is to generate a large quantity of questions.
* Each new question expose you to a new aspect and increase your chance of making a discovery.
* Two types of questions will always be useful:
    * What kind of variation occur within my variable?
    * what type of covariation occurs between my variables?

* Variable: quantity/quality/property that you can measure
* Value: state of a variable when you measure it
* Observation: a set of measurements. An observation contain several values, each
  associated with a different variable. Will also refer as data points.
* Tabular data: a set of values, each associated with a variable and an observation.

## 7.3 Variation

* The best way to understand variation is to visualize distribution.

### 7.3.1 Visualising distributions

* To examine the distribution of a categorical variable, use a bar chart.
    * visual: `geom_bar`, data, `count`
* To examine the distribution of a continuous variable, use a histogram.
    * visual: `geom_histogram`, data, `count(cut_width())`
* To overlay multiple histograms, use `geom_freqpoly` and use maybe `mapping = aes(colour = cut)`
* Now we know the variation, what's next? Good questions will be to rely on your curiosity
  and skepticism.
  
### 7.3.2 Typical values

* Which values are the most common? Why?
* Which values are rare? Why? Does that match your expectations?
* Can you see any unusual patterns? What might explain them?

### 7.3.3 Unusual values

* Sometimes they are data entry errors
* Sometimes outliers suggest important new science.
* The outliers might be hard to see in a histogram.
    * Use `coord_cartesian(xlim = , ylim = )`
    * The `xlim()` and `ylim()` in ggplot2 throws away the data outside the limit.
* If outliers have substantial impact on your data, we shouldn't remove them
  without justification. If we need to figure out what caused them and disclose them.
  
## 7.4 Missing values

* 2 ways to deal with strange values
* Drop the entire row: `filter(between(y, 3, 20))`, not recommended
* Mark them as `NA`: `mutate(y = ifelse(y < 3 | y > 20, NA, y))`, recommended
* `ggplot` won't let the `NA` go silently, will give warnings. `na.rm = TRUE` to suppress.

## 7.5 Covariation

* The best way to spot covariation is to visualize the relationship between two or more variables.

### One continuous and 1 categorical

* `geom_freqpoly` is not that useful because if one group is much smaller than the other, then
  it's hard to see.
* The better way is to display density instead of count
* Another way is to use boxplot
    * A box stretches from 25% to 75% (IQR)
    * There is a line in the middle for median
    * Visual points for 1.5 of IQR
    * A line for non-outlier
* For non-ordered categorical variable, can use `reorder` to order them:
  `reorder(class, hwy, FUN = median)`

# Tricks and tips from R4DS

* `Cmd+Alt+P`: resend the last executed chunk
* "https://twitter.com/rstudiotips" to look for tips.
* "https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics" to find out 
  common mistakes Rstudio will report.

