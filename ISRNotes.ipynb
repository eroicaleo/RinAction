{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 01\n",
    "\n",
    "The first thing to do is **not** running through\n",
    "a fancy algorithm, make some graph and plots first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Accurate predict unseen test cases\n",
    "* Understand which inputs affect outputs and how\n",
    "* Access the quality of our predictions and inferences\n",
    "\n",
    "## Philosophy\n",
    "\n",
    "* Know when and how to use what techniques\n",
    "* Understand simple methods first and then sophisticated ones\n",
    "* Accurately assess the performance of a method, how well and how badly\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "* No outcome\n",
    "* Fuzzy objective\n",
    "* can be useful as pre-processing step for supervised learning\n",
    "\n",
    "## Statistical Learning V.S. Machine Learning\n",
    "\n",
    "* ML: large scale applications and prediction accuracy\n",
    "* SL: Models and their interpretability, precision and uncertainty??\n",
    "\n",
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "* response: Y; feature, input: X\n",
    "\n",
    "$$Y = f(X)+\\epsilon$$\n",
    "\n",
    "## What is f(X) good for\n",
    "\n",
    "* predict on new points\n",
    "* Can tell Which components are important\n",
    "* Maybe can tell how each component affects Y\n",
    "\n",
    "## The regression function f(x)\n",
    "\n",
    "$$f(x) = f(x_1, x_2, x_3) = E(Y|X=x)$$\n",
    "\n",
    "* Reducible and Irreducible: Var(e)\n",
    "\n",
    "## How to estimate f(x)\n",
    "\n",
    "* Use neighborhood of x: N(x)\n",
    "\n",
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curse of dimension\n",
    "\n",
    "* The drawback of the above method:\n",
    "* We want to reduce variance, so choose certain percent of data points, say 10%.\n",
    "* Whe dimension p is large, the radius can be large\n",
    "\n",
    "## Parametric and structured models\n",
    "\n",
    "* Linear model, quadratic model\n",
    "* thin-plate splines averaging (later ch. 7), can be overfitting with bad parameter.\n",
    "\n",
    "## Trade-off\n",
    "\n",
    "* prediction accuracy v.s. interpretability\n",
    "* good v.s. under v.s. over fit\n",
    "* Parsimony (simpler model) v.s. blackbox (use all variables)\n",
    "\n",
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Model Accuracy\n",
    "\n",
    "average squared prediction error\n",
    "\n",
    "* on training data, then biased towards overfit model\n",
    "* on testing data, better\n",
    "\n",
    "## Bias-Variance Trade-off\n",
    "\n",
    "$$E(y-\\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + var(\\epsilon)$$\n",
    "\n",
    "Note that x_0 is a test data, why f hat x_0 is a random varible?\n",
    "Here, method is fix but training data can vary according some probablity.\n",
    "\n",
    "So bias is defined as the following:\n",
    "\n",
    "$$E(\\hat{f}(x_0)) - f(x_0)$$\n",
    "\n",
    "More flexible model, less bias but more variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Neighborhood approach can also work for low dimension.\n",
    "\n",
    "* SVM build structure model for C(X), the classifier\n",
    "* logistic regression, generalized additive model, build the structure model for the probability p_k(x)\n",
    "\n",
    "## Nearest  Neighbor\n",
    "\n",
    "K small, high variance, K large, high bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "* Simple is good, useful both in conceptually and practically\n",
    "\n",
    "## Question to ask (Applicable to all algorithm I think)\n",
    "\n",
    "* Is there a relation between a predictor and the response\n",
    "* How strong is the relationship?\n",
    "* How accurately can we predict future?\n",
    "* Is the relationship linear?\n",
    "* Is there synergy among predictor?\n",
    "\n",
    "## Accessing the accuracy of the coefficient estimates\n",
    "\n",
    "* SE(\\beta_1) depends on and reversely depends on the spread of X.\n",
    "    * When we select X, it's good to spread X to get good extimation of slope.\n",
    "* Confidence interval 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.2\n",
    "\n",
    "## Hypothesis testing\n",
    "\n",
    "Null hypothesis: There is no relationship between X and Y, H_0\n",
    "Alternative hypothesis: There is some relation.\n",
    "\n",
    "* Compute t-statistics\n",
    "* Then compute p-value\n",
    "\n",
    "## Assessing the overall accuracy of the model\n",
    "\n",
    "* Residual standard error\n",
    "* R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.3\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "* A regression coefficient beta_j estimates the expected change in Y per unit\n",
    "  change in X_j, with all other predictors held fixed. BUT predictors usually change together\n",
    "\n",
    "* The only way to find out what will happen when a complex system is disturbed\n",
    "  is to disturb the system, not merely to observe it passively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3.4\n",
    "\n",
    "### Some important questions\n",
    "\n",
    "1. Is at least one of the predictors useful in predicting the response?\n",
    "    * Look at F-statistics, the bigger the better\n",
    "2. Do all the predictors help to explain Y, or is only a subset of the predictors useful?\n",
    "    * examine all subsets, not feasible for big p\n",
    "    * Forward selection\n",
    "    * Backward selection\n",
    "    * Other method: AIC, BIC, cross validation\n",
    "3. How well does the model fit the data?\n",
    "4. Given a set of predictor values, what response value should we predict, and how accurate?\n",
    "\n",
    "### Qualitative variables\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i $$\n",
    "\n",
    "x_i can take 1 or 0.\n",
    "\n",
    "If a qualitative variable is more than 2 level, needs to create multiple variables.\n",
    "One for if Asian or not, one for if Caucasian or not.\n",
    "One fewer variable than number of levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3.5 Interaction and nonlinearity\n",
    "\n",
    "* Use product of 2 predictors\n",
    "\n",
    "### Hierarchy\n",
    "\n",
    "* If we want to include interactions, we will also need to put main effects\n",
    "\n",
    "### Uncovered parts\n",
    "\n",
    "section 3.33\n",
    "Outliers\n",
    "Non-constant variance of error terms\n",
    "High leverage points\n",
    "Collinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 8.1\n",
    "\n",
    "Tree based methods: stratifying and segmenting predictor space into a number simple regions.\n",
    "Simple and easy to explain. Accuracy can be improved by bagging, random forest and boosting.\n",
    "\n",
    "### Terminology\n",
    "\n",
    "* Divided region is called terminal nodes\n",
    "* Tree is upside down, so leaves are at the bottom\n",
    "* The points splitting the input space is internal node\n",
    "\n",
    "### Tree building process\n",
    "\n",
    "* Divide the predictor space into non-overlapping regions R1 ... Rj.\n",
    "* Everything in the same region, same prediction.\n",
    "* Although the region can be any shape, but boxes are easy to interpret.\n",
    "* Optimal is infeasible.\n",
    "* Top down and greedy approach, i.e. select best variable to best split which results to best RSS reduction.\n",
    "* The results are piecewise constant\n",
    "\n",
    "### Pruning a tree\n",
    "\n",
    "* Large tree: overfit and poor test data.\n",
    "* Better strategy: build a large tree and prune it back\n",
    "    * Cost complexity pruning, penalize with tree size (i.e. # of terminal nodes) with alpha.\n",
    "* **Looks like the lecture does not cover the details on how to prune the tree**\n",
    "    \n",
    "### Tree algorithm\n",
    "\n",
    "1. Recursive binary splitting to grow large tree, stop when terminal nodes have less observations\n",
    "2. Prune the tree based on cost complexity pruning for a sequence of alpha.\n",
    "3. Use the K-fold cross validation to choose alpha.\n",
    "4. Go back to step 2 and choose the tree.\n",
    "\n",
    "### Regression tree\n",
    "\n",
    "Hitter's data:\n",
    "    * Number of years + Number of hits to predict\n",
    "    * Internal node is a split of one predictor\n",
    "    * The prediction is the mean of obsvervations in the leaf node.\n",
    "\n",
    "### Classification tree"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
